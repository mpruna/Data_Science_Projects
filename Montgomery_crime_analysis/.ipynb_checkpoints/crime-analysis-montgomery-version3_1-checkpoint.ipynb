{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Montgomery County crimes to find useful patterns\n",
    "\n",
    "Montgomery is a county in the Maryland US located on the east coast [Montgomery County, Maryland](https://en.wikipedia.org/wiki/Montgomery_County,_Maryland). \n",
    "\n",
    "Improvement of various aspects of social life entitles a proactive and reactive analysis. \n",
    "With this in mind I will be looking to to find patterns by performing time, location and crime classification analysis. For this project I will heavily rely on graph visualization as a picture worths a thousand words.\n",
    "\n",
    "Here are some conclusion highlights:\n",
    "1. Time analysis:\n",
    "    1. Most of the crimes are committed Tuesday\n",
    "    2. On 24 hour basis most of the crimes are committed between 7 a.m - 11 p.m\n",
    "    3. October has the highest crime count\n",
    "2. Classification analysis:\n",
    "    1. Violent/Non-Violent crimes rates are pretty even 42.8%/57.2%\n",
    "3. Location analysis:\n",
    "    1. Cities with highest crime counts are : Silver Spring, Rockville, Gaithersburg\n",
    "    2. Most of the crimes happen in the street, residence or parking lot\n",
    "    3. Silver Spring Police District has the highest crime rates\n",
    "\n",
    "Dataset for this project: \n",
    "[here](https://data.montgomerycountymd.gov/Public-Safety/Crime/icn6-v9z3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'MontgomeryCountyCrime2013.csv' does not exist: b'MontgomeryCountyCrime2013.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8dea61701043>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcrimes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MontgomeryCountyCrime2013.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mcrimes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anaconda3/envs/python_37_env/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anaconda3/envs/python_37_env/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anaconda3/envs/python_37_env/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anaconda3/envs/python_37_env/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anaconda3/envs/python_37_env/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'MontgomeryCountyCrime2013.csv' does not exist: b'MontgomeryCountyCrime2013.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "crimes = pd.read_csv(\"MontgomeryCountyCrime2013.csv\")\n",
    "crimes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the data\n",
    "\n",
    "Each row in the dataset represents a crime being commited. Data contains location information, crime classification as well as various timestamps.\n",
    "In examining the dataset our goals would be to:\n",
    "1. Find the columns that have meaningfull information, have minimum missing values, and also hold granular data.\n",
    "2. We also need to perform data cleaning/manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert `Start Date / Time`; `End Date / Time`; `Dispatch Date/ Time` to datetime format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the number of (rows,columns) from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1. Main takeaway here is that `End Date / Time` has a high number of missing value, for this reason it can't be used for our analysis \n",
    "2. A deeper look into the collumns that have missing values is needed, also we have to determine which columns will provide usefull insight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep=['Zip Code','Sector','Beat','PRA','Latitude','Longitude','Location','Address Number']\n",
    "\n",
    "for col in columns_to_keep:\n",
    "    item_null=crimes[col].notnull()\n",
    "    print(col+\"\\n\",crimes[col][item_null==True].head(),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns analysis:\n",
    "\n",
    "1. `Zip Code`; `Sector`; `Beat`; `Address Number` can't be used for our analysis as these details are meaningless for a casual reader.\n",
    "2. Columns like `Dispatch Date / Time`; `Class Description`; `City`; `Start Date / Time`; `Police District Number` will be useful\n",
    "3. `Latitude` and `Longitude` is not an obvious choice, at least from a comprehensibility standpoint but will be useful for map visualization later on.\n",
    "4. Also the number of nan values for `End Date / Time` column is quite high so for this reason I will choose `Dispatch Date / Time` instead\n",
    "5. I will exclude missing latitude, longitude values from our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exclude lat&lon missing values\n",
    "lat_null=crimes['Latitude'].notnull()\n",
    "lon_null=crimes['Latitude'].notnull()\n",
    "crimes=crimes[(lat_null==True) & (lon_null==True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time analysis\n",
    "\n",
    "The aim here is to spot time related patterns in crimes. Time analysis is structured around these questions:\n",
    "\n",
    "1. What day of the week are the most crimes committed on? (i.e Monday, Tuesday, etc)\n",
    "2. During what time of day are the most crimes committed?\n",
    "3. During what month are the most crimes committed?\n",
    "\n",
    "First step would be to convert the `Dispatch Date / Time` column from an object type to a datetime type.<br>\n",
    "Dispatch Date/Time--The actual date and time a Officer was dispatched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datetime modules\n",
    "import datetime as dt\n",
    " \n",
    "# convert 'Dispatch Date / Time' to datetime format\n",
    "crimes['Dispatch Date / Time']=pd.to_datetime(crimes['Dispatch Date / Time'])\n",
    "\n",
    "# get crime counts/weekday;hour;month\n",
    "crimes['Dispatch Day of the Week']=crimes['Dispatch Date / Time'].dt.weekday_name\n",
    "crimes['Dispatch Hour']=crimes['Dispatch Date / Time'].dt.hour\n",
    "crimes['Dispatch Month']=crimes['Dispatch Date / Time'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotting modules\n",
    "\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, iplot\n",
    "from plotly.graph_objs import *\n",
    "init_notebook_mode()\n",
    "import cufflinks as cf\n",
    "\n",
    "# day of the week crime counts\n",
    "dow=crimes['Dispatch Day of the Week'].value_counts().copy()\n",
    "\n",
    "# Converting to a series to a dataframe\n",
    "# It will be easier for plottling\n",
    "pd.DataFrame({'Dispatch Day of the Week':dow.index,'Counts':dow}).reset_index(drop=True)\n",
    "\n",
    "# plotting\n",
    "dow.iplot(theme='pearl', filename='crime_distrib_per_day', title='Crime distribution per day of the week',\n",
    "         xTitle='Day', yTitle='Count', world_readable=False)\n",
    "dow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day of the week analysis:\n",
    "\n",
    "1. Generally it seems that less crimes are committed towards the weekend and highest counts are around the beginning of the week\n",
    "2. Crime peak is Tuesday and lowest crimes are on Sunday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hour crime counts\n",
    "tod=crimes['Dispatch Hour'].value_counts()\n",
    "\n",
    "# Converting to a series to dataframe\n",
    "pd.DataFrame({'Dispatch Hour':tod.index,'Counts':tod}).reset_index(drop=True)\n",
    "\n",
    "# Pandas dataframe needs to be sorted by index for our time analysys otherwise the graph will be scrambled.\n",
    "# This is because df will be sorted by values\n",
    "tod.sort_index().iplot(theme='pearl', filename='crime_distrib_per_hour', title='Crime distribution per hour',\n",
    "         xTitle='Hour', yTitle='Count', world_readable=False)\n",
    "\n",
    "tod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crime analysis by hour:\n",
    "1. Highest value is a 7 am and lowest is at 5 am.\n",
    "2. Most of the crimes are committed between 7 am and 11 pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crime distribution/month\n",
    "tom=crimes['Dispatch Month'].value_counts()\n",
    "\n",
    "# convert series to df\n",
    "pd.DataFrame({'Dispatch Month':tom.index,'Counts':tom}).reset_index(drop=True)\n",
    "\n",
    "# sort index & plot\n",
    "tom.sort_index().iplot(theme='pearl', filename='crime_distrib_per_month', title='Crime distribution per month',\n",
    "         xTitle='Month', yTitle='Count', world_readable=False)\n",
    "\n",
    "tom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Month crime analysis:\n",
    "\n",
    "1. Data is not complete, we only have the statistics from July till the end of the year.\n",
    "2. July month has the least crimes committed followed by a substantial increase in crimes starting from August.\n",
    "3. I can attribute this to vacation time. Most probably by the end of August most people return from vacation. Peak is in October"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dispatch time interval analysis\n",
    "\n",
    "Dispatch time could be a strong indicator as to how does police prioritize crimes. \n",
    "1. To do this we need to see the general time difference between `Dispatch Date / Time` and  `Start Date / Time`\n",
    "2. This will be useful to determine and categorize the dispatch time intervals\n",
    "3. Also it would be interesting to see if there is any difference in dispatch time based on the type of incidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to datetime\n",
    "crimes['Start Date / Time']=pd.to_datetime(crimes['Start Date / Time'])\n",
    "\n",
    "# get difference between 'Dispatch Date / Time' 'Start Date / Time'\n",
    "crimes['Date diff']=crimes['Dispatch Date / Time'] -crimes['Start Date / Time']\n",
    "pd.DataFrame(crimes.groupby(['Start Date / Time','Dispatch Date / Time'])['Date diff'].value_counts().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. As we can see start time column is not accurate because a lot of dates that have the start time before 2013:\n",
    "2. The data sample we are analyzing  provides a summary of incidents from 07/2013 till 12/2013\n",
    "3. So we will have to ignore those rows from out analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude records where start time was before 2013/07/01\n",
    "crimes_slice=crimes[(crimes['Start Date / Time'] >= dt.date(2013,7,1))].copy()\n",
    "\n",
    "# group Start/Dispatch Time on minutes intervals [<1,(1,5),(5,10),(10,30),(30,60),>60]\n",
    "crimes_slice.loc[crimes_slice['Date diff']<dt.timedelta(minutes=1),'Less then 1']='True'\n",
    "crimes_slice.loc[((crimes_slice['Date diff']>dt.timedelta(minutes=1))& (crimes_slice['Date diff']<dt.timedelta(minutes=5))),'Between 1 and 5']='True'\n",
    "crimes_slice.loc[((crimes_slice['Date diff']>dt.timedelta(minutes=5)) & (crimes_slice['Date diff']<dt.timedelta(minutes=10))),'Between 5 and 10']='True'\n",
    "crimes_slice.loc[((crimes_slice['Date diff']>dt.timedelta(minutes=10)) & (crimes_slice['Date diff']<dt.timedelta(minutes=30))),'Between 10 and 30']='True'\n",
    "crimes_slice.loc[((crimes_slice['Date diff']>dt.timedelta(minutes=30)) & (crimes_slice['Date diff']<dt.timedelta(minutes=60))),'Between 30 and 60']='True'\n",
    "crimes_slice.loc[crimes_slice['Date diff']>dt.timedelta(minutes=60),'Above 60']='True'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing the df on the time interval collumns & get the counts\n",
    "td=crimes_slice.loc[:,['Less then 1',\n",
    "                    'Between 1 and 5',\n",
    "                    'Between 5 and 10',\n",
    "                    'Between 10 and 30',\n",
    "                    'Between 30 and 60',\n",
    "                    'Above 60']].apply(pd.Series.value_counts)\n",
    "\n",
    "\n",
    "#.iloc[0]  select that row positionally using iloc, which gives you a Series with the columns as the new index and values, \n",
    "# then sorting by values\n",
    "td.iloc[0].sort_values(ascending=False).iplot(kind='bar', title='Crime distribution Start / Dispatch Time',\n",
    "         xTitle='Time Delta/minutes', yTitle='Count',color='rgba(55, 128, 191, 1.0)',filename='cufflinks/bar-chart-row')\n",
    "\n",
    "td.iloc[0].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dispatch time interval breakdown:\n",
    "\n",
    "1. The majority of the dispatch time happened quite fast(less then a minute). However this could be misleading as there isn't a clear description of what this column represent. I would tend to believe that `Dispatch Date / Time` represents the time when an officer was sent to a crime location not when it arrived at the crime scene.\n",
    "2. Next would be the response time above 60 minutes.\n",
    "3. Also it's important to remember that this is only a slice of the dataset as some start time might have not been accurate.\n",
    "4. Researching  the internet I saw various sources and it seems that response time varies between 6 and 15 minutes for critical issues. However I could not find an universal SLA for dispatching police officers.\n",
    "5. Also another point worth taking into consideration that not all incidents require the same attention and some of the could be resolved over the phone.\n",
    "6. The same analysis would be interesting when diving further into our analysis and classifying the crimes in violent/non violent ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dispatch Time Delta analysis\n",
    "\n",
    "1. The bar chart above is not to revealing in term of quantifying the timedeltas\n",
    "2. If we want to do that we will need to use a pie chart. For simplicity I will have to convert the series to a dataframe format for a pie chart with cufflinks. See below:\n",
    "<br>\n",
    "https://plot.ly/pandas/pie-charts/\n",
    "<br>\n",
    "![pie format](pie_format.png \"Title\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert series to dataframe\n",
    "td_pie=td.iloc[0].to_frame()\n",
    "\n",
    "# reset index, if column is in index it can't be used to plot  \n",
    "td_pie=td_pie.reset_index()\n",
    "\n",
    "# rename columns, this step is not necessary\n",
    "td_pie.columns=['Time Deltas','Values']\n",
    "\n",
    "# plot\n",
    "td_pie.iplot(kind='pie',labels='Time Deltas',values='Values',pull=.1,hole=.2,title='Dispatch Time interval breakdown',\n",
    "                   textposition='outside',textinfo='value+percent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Dispatch time intervals percentage breakdown:\n",
    "1. dispatch time < 1 min             48.4%\n",
    "2. dispatch time > 1 min & < 5 min   1.72%\n",
    "3. dispatch time > 5 min & < 10 min  2.2%\n",
    "4. dispatch time > 10 min & < 30 min 4.37%\n",
    "5. dispatch time > 30 min & < 60 min 2.24%\n",
    "6. dispatch time > 60 min            41%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing crime locations:\n",
    "\n",
    "1. Will use the following criteria for choosing columns:\n",
    "    1. Granularity: Small areas shouldn't be used, because only a few crimes were committed inside them, which makes it hard to analyze and compare\n",
    "    2. Comprehensibility: Need to analyze data that is compelling for the casual reader\n",
    "    3. Missing values: If a column has a lot of missing values, that means that the conclusions you draw are less valid, because you don't know if the missing data is systematic\n",
    "\n",
    "\n",
    "2. Columns used:\n",
    "    1. Police District Number | Major Police Boundary corresponding to Police District Names i.e (Rockville,Weaton etc.)\n",
    "    2. City | City "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crime distribution by City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crime counts per City, transform to df; sort\n",
    "\n",
    "crimes_city=crimes['City'].value_counts().to_frame().sort_values(by='City')\n",
    "\n",
    "# graph layout\n",
    "layout=dict(autosize= True,\n",
    "            title = 'Crime counts per City / top 15',\n",
    "            xaxis=dict(domain=[0.08, 1]),     \n",
    "            )\n",
    "\n",
    "# plot\n",
    "crimes_city.tail(15).iplot(kind='barh',barmode='normal', bargap=.8, filename='cufflinbarh',layout=layout)\n",
    "crimes_city.sort_values(by='City',ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crime distribution by location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get crime counts/place; transform to df\n",
    "crimes_place=crimes['Place'].value_counts().to_frame().sort_values(by='Place')\n",
    "\n",
    "# graph layout\n",
    "layout=dict(\n",
    "            title = 'Crime counts per Place / top 15',\n",
    "            xaxis=dict(domain=[0.2, 1]),\n",
    "            yaxis=dict(domain=[0.1, 0,66])    \n",
    ")\n",
    "\n",
    "# plot\n",
    "crimes_place.tail(15).iplot(kind='barh',barmode='normal', bargap=.4, filename='cufflinbarh',layout=layout)\n",
    "crimes_place.sort_values(by='Place',ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analize crimes based on Police District Location\n",
    "\n",
    "Although Police District does not mean much for the casual reader it could provide useful insight into our data analysis as the area are quite big an would provide great granularity.<br>\n",
    "Let's visualize Police District Location:<br>\n",
    "![Montgomery County map](Countywidemap.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PD summary:<br>\n",
    "    1D | Rockville<br>\n",
    "    2D | Bethesda<br>\n",
    "    3D | Silver Spring<br>\n",
    "    4D | Wheaton<br>\n",
    "    5D | Germantown<br>\n",
    "    6D | Gaitersburg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot crime distribution/ PD\n",
    "crimes['Police District Number'].value_counts().iplot(kind='bar',title='Crime distribution  / Police District Number',\n",
    "        xTitle='Police District Number', yTitle='Count',color='rgba(55, 128, 191, 1.0)', filename='crime_distribution_per_district')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Crime by PD:\n",
    "\n",
    "Silver Spring has the highest crime rates, and the lowest crime rates are in Germantown. Because of the lack of data we will exclude the last district i.e TPPD from our analysis<br>\n",
    "This is the order of the crimes counts / per district from the highest to lowest:<br>\n",
    "    1. Silver Spring; Wheaton; Gaithersburg; Rockville; Bethesda; Germantown\n",
    "But this would not be an accurate analysis unless we take into account the census data for these districts.<br>\n",
    "For this I will use the census data for Montgomery county Police Department. See below link at the end there is census data per each district.\n",
    "https://www.wau.edu/wp-content/uploads/2012/09/MCPCrimeReport2014.compressed.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing a pandas dataframe with Police District Number & Population\n",
    "census = pd.DataFrame([{'Police District Number':'1D','Population':'149118'},\n",
    "                       {'Police District Number':'2D','Population':'182883'},\n",
    "                       {'Police District Number':'3D','Population':'152991'},\n",
    "                       {'Police District Number':'4D','Population':'208263'},\n",
    "                       {'Police District Number':'5D','Population':'131391'},\n",
    "                       {'Police District Number':'6D','Population':'147486'}]\n",
    "                      )                     \n",
    "\n",
    "# crime counts / Police District Number\n",
    "cpd=crimes['Police District Number'].value_counts()\n",
    "\n",
    "# Change the series to a df object with PD as index\n",
    "cpd=pd.DataFrame({'Police District Number':cpd.index,'Counts':cpd}).reset_index(drop=True)\n",
    "\n",
    "# Merge the 2 dfs\n",
    "result=pd.merge(census, cpd, on='Police District Number')\n",
    "\n",
    "# Compute the crime counts pe 100k,\n",
    "result['Counts/100k']=100000*result['Counts']/pd.to_numeric(result['Population'], errors='coerce')\n",
    "\n",
    "# Set Police district number as index, \n",
    "# this is needed to have on the X axis the Police District Number, then get the value_count()\n",
    "result.set_index('Police District Number',inplace=True)\n",
    "result['Counts/100k'].sort_values(ascending=False).iplot(kind='bar',title='Crime distribution per 100K',xTitle='Police District Number',yTitle='Counts/100K')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Crime by PD/100k:\n",
    "\n",
    "Silver Spring is still the district with the highest crime rates, followed by Gaithersburg and at the end Bethesda.<br>\n",
    "This is the order from highest to the lowest:<br>\n",
    "    1. Silver Spring; Gaithersburg; Rockville; Wheaton; Germantown; Bethesda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crime distribution/PD vizualization pie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the crime percentage per district\n",
    "result['Percentage']=result['Counts']*100/pd.to_numeric(result['Population'], errors='coerce')\n",
    "\n",
    "#Need to reset the index otherwise I can't plot a pandas df and use one column if that column is in the index\n",
    "result.reset_index().iplot(kind='pie',labels='Police District Number',values='Percentage',title=\"Crime percentage distribution / District\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crime percentage breakdown/PD:\n",
    "* 3D - Silver Spring 24.8%\n",
    "* 6D - Gaitersburg 17.6%\n",
    "* 1D - Rockville 16%\n",
    "* 4D - Wheaton 14.5%\n",
    "* 5D - Germantown 14.3%\n",
    "* 2D - Bethesda 12.7%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crime analysis by City & PD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group crimes based on Police District Number / City; get the value_counts\n",
    "pd_city_series=crimes.loc[:,['Police District Number','City']].groupby('Police District Number')['City'].apply(lambda s: s.value_counts())\n",
    "\n",
    "# get the names of cities that appear on different PDs\n",
    "# convert series to frame; reset index; rename the columns\n",
    "pd_city=pd_city_series.to_frame()\n",
    "pd_city.reset_index(inplace=True)\n",
    "pd_city.columns=['Police District Number','City','Counts']\n",
    "pd_city_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing that stick out is that some cities appear on several districts. This could be due to crime locations being close to multiple PDs.<br>\n",
    "Let's do a map visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get duplicated cities \n",
    "pd_city_dup=pd_city[pd_city.duplicated('City')==True]['City'].unique()\n",
    "pd_city_dup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## City map visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "map_1 = folium.Map(location=[39.154743, -77.240515],\n",
    "                   zoom_start=9.7,\n",
    "                   tiles='Stamen Terrain')\n",
    "folium.Marker([39.0838889, -77.1530556], popup='ROCKVILLE').add_to(map_1)\n",
    "folium.Marker([39.0180556, -77.2088889], popup='POTOMAC').add_to(map_1)\n",
    "folium.Marker([38.9905556, -77.0263889], popup='SILVER SPRING').add_to(map_1)\n",
    "folium.Marker([38.9712215, -77.0763667], popup='CHEVY CHASE').add_to(map_1)\n",
    "folium.Marker([39.0256651, -77.0763669], popup='KENSINGTON').add_to(map_1)\n",
    "folium.Marker([39.1142747, -76.9783097], popup='SPENCERVILLE').add_to(map_1)\n",
    "folium.Marker([39.1433333, -77.2016667], popup='GAITHERSBURG').add_to(map_1)\n",
    "folium.Marker([39.1730556, -77.2719444], popup='GERMANTOWN').add_to(map_1)\n",
    "folium.Marker([39.1837171, -77.3127623], popup='BOYDS').add_to(map_1)\n",
    "folium.Marker([39.11733,  -77.1610916], popup='DERWOOD').add_to(map_1)\n",
    "folium.Marker([39.1806623, -77.0591452], popup='BROOKEVILLE').add_to(map_1)\n",
    "folium.Marker([39.0180556, -77.2088889], popup='POOLESVILLE').add_to(map_1)\n",
    "folium.Marker([39.1766667, -77.1955556], popup='MONTGOMERY VILLAGE').add_to(map_1)\n",
    "folium.Marker([39.1530556, -77.0672222], popup='OLNEY').add_to(map_1)\n",
    "folium.Marker([38.9777778, -77.0077778], popup='TAKOMA PARK').add_to(map_1)\n",
    "\n",
    "map_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because it's not clear why a city name appears on multiple PDs, I will focus the analysis on other areas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze crime by location:\n",
    "1. determine what are the most and least common crime locations(i.e residence,street) for Montgomery county and for each PD. \n",
    "2. get 10 most common/least common locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import graph modules\n",
    "import plotly.graph_objs as go\n",
    "from plotly import tools\n",
    "\n",
    "# Will get the total number of crimes committed\n",
    "total=crimes['Place'].count()\n",
    "\n",
    "# Get the value_counts() for all the places\n",
    "place=crimes['Place'].value_counts()\n",
    "\n",
    "# Convert the series to a df\n",
    "place=pd.DataFrame({'Place':place.index,'Counts':place}).reset_index(drop=True)\n",
    "\n",
    "# Get crime percentege on a given location.\n",
    "place['Percent']=place['Counts']*100/total\n",
    "\n",
    "# 10 most/least common\n",
    "trace1=go.Bar(x=place['Place'].head(10),y=place['Percent'].head(10),name='most common')\n",
    "trace2=go.Bar(x=place['Place'].tail(10),y=place['Percent'].tail(10),name='least common')\n",
    "\n",
    "fig = tools.make_subplots(rows=1, cols=2, subplot_titles=('Montgomery 10 most common crime locations', \n",
    "                                                          'Montgomery 10 least common crime locations'))\n",
    "\n",
    "fig.append_trace(trace1,1,1)\n",
    "fig.append_trace(trace2,1,2)\n",
    "\n",
    "fig['layout'].update(height=500,width=1000,autosize=True,margin=go.Margin(b=135),title='Montgomery crime percentage by location')\n",
    "iplot(fig, filename='stacked-subplots-shared-xaxes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 most common locations:\n",
    "1. Residence (Single Family; Apartment; Townhouse/Duplex; Driveway)\n",
    "2. Street(in Vehicle; Residential; Commercial)\n",
    "3. Parking Lot (Commercial; Residential)\n",
    "\n",
    "### 10 least common locations:\n",
    "1. Jail\n",
    "2. Retail(Dry Cleaner; Video Store)\n",
    "3. Liquor Store\n",
    "4. Lake \n",
    "5. Pawn Shop\n",
    "6. Residence(Mobile Home)\n",
    "6. Nursery\n",
    "7. Parking Lot(Park & Ride)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  10 most/least common crime locations on PDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First I will create a df where I will group the df by 'Police District Number' chain it to 'Place' column and get\n",
    "# the value_counts(), and create a new column called Counts\n",
    "crimes_district_location=pd.DataFrame({'Counts' : crimes.groupby(['Police District Number'])['Place'].value_counts()}).reset_index()\n",
    "\n",
    "def find_pcent(row):\n",
    "    # Get the total crimes per district\n",
    "    p_d_val_counts = crimes['Police District Number'].value_counts()\n",
    "    \n",
    "    # get a list of unique police district numbers and iterate over it\n",
    "    for p_d in crimes['Police District Number'].unique():\n",
    "        # for each Police District Number get the percentage\n",
    "        if row['Police District Number'] == p_d:\n",
    "            return (row['Counts'] / p_d_val_counts[p_d]) * 100\n",
    "\n",
    "# Use an apply function to compute the percentage for each PD / location        \n",
    "crimes_district_location['%'] = crimes_district_location.apply(find_pcent, axis = 1)\n",
    "crimes_district_location.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting functions\n",
    "\n",
    "1. to make the code modular I decided to implement 3 plotting functions\n",
    "2. These are the benefits:\n",
    "    1. reduce the overall code\n",
    "    2. put emphasis on the plot not the plotting code\n",
    "    3. focus more on the actual data behind the plot\n",
    " \n",
    " \n",
    "3. plot_bar_staked:\n",
    "\n",
    "    1. Official doc: https://plot.ly/python/bar-charts/\n",
    "    2. plotting will be done based on a data list\n",
    "    3. data is comprised of traces\n",
    "    4. a trace is made out of:\n",
    "        1. labels (x on the axis) \n",
    "        2. values (y on the axis)\n",
    "        3. name of the particular trace\n",
    "    5. particular to this code we will plot data based on top/least values\n",
    "    6. so that means we will have separate data and traces for each case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bar_staked(df,part):\n",
    "    data_h=[]\n",
    "    data_t=[]\n",
    "    names=['PD_1D','PD_2D','PD_3D','PD_4D','PD_5D','PD_6D']\n",
    "    for pd,n in zip(['1D','2D','3D','4D','5D','6D'],names):\n",
    "        data_h+=['trace'+str(pd)]\n",
    "        data_t+=['trace'+str(pd)]\n",
    "        labels_h=crimes_district_location[crimes_district_location['Police District Number']==pd]['Place'].head()\n",
    "        labels_t=crimes_district_location[crimes_district_location['Police District Number']==pd]['Place'].tail()\n",
    "        values_h=crimes_district_location[crimes_district_location['Police District Number']==pd]['%'].head()\n",
    "        values_t=crimes_district_location[crimes_district_location['Police District Number']==pd]['%'].tail()\n",
    "        data_h[len(data_h)-1]=go.Bar(x=labels_h,y=values_h,name=n)\n",
    "        data_t[len(data_t)-1]=go.Bar(x=labels_t,y=values_t,name=n)\n",
    "    \n",
    "    if part=='head':\n",
    "        return data_h\n",
    "    elif part=='tail':\n",
    "        return data_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting most common locations\n",
    "# 1) need to pass to the plotting function data for the analysis in this case 'crimes_district_location;\n",
    "# 2) need to specify if are are looking for top/least common locations in this case top most i.e part='head'\n",
    "\n",
    "df=crimes_district_location\n",
    "part='head'\n",
    "data=plot_bar_staked(df,part)\n",
    "\n",
    "# create figure layout\n",
    "layout = go.Layout(\n",
    "    xaxis=dict(tickangle=-45),\n",
    "    barmode='stack',\n",
    "    autosize=True,\n",
    "    margin=go.Margin(b=160),\n",
    "    title='5 most common crime locations accross all Police Districts'\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "iplot(fig, filename='angled-text-bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most common crime locations:\n",
    "1. Residence - Single Family\n",
    "2. Street - In vehicle\n",
    "3. Residence - Apartment/Condo\n",
    "4. Street - Residential\n",
    "5. Other\n",
    "6. Parking Lot\n",
    "7. Residence - Townhouse/Duplex\n",
    "8. Residence - Driveway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create plot trace for each district; get 5 least common locations\n",
    "df=crimes_district_location\n",
    "part='tail'\n",
    "data=plot_bar_staked(df,part)\n",
    "\n",
    "#create fig layout\n",
    "layout = go.Layout(\n",
    "    xaxis=dict(tickangle=-45),\n",
    "    barmode='stack',\n",
    "    autosize=True,\n",
    "    margin=go.Margin(b=140),\n",
    "    title='5 least common crime locations accross all Police Districts'\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "iplot(fig, filename='angled-text-bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least common crime locations:\n",
    "* Liquer Store\n",
    "* Theater\n",
    "* Pawn Shop\n",
    "* Retail (Jewelery;Video Store; Dry Cleaner;Hardware)\n",
    "* Nursery\n",
    "* Parking Lot (Church;Park & Ride;Rec Center; Metro)\n",
    "* Residence (Mobile Home; Carpot) \n",
    "* Loundromat\n",
    "* Check Cashing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analize violent/non-violent crimes:\n",
    "According to the UCR(Uniform Crime Reporting) definition violent crimes are the following:<br>\n",
    "    \"The descending order of UCR violent crimes are murder and nonnegligent manslaughter, forcible rape, robbery, and \n",
    "aggravated assault, followed by the property crimes of burglary, larceny-theft, and motor vehicle theft. Although arson is also a property crime, the Hierarchy Rule does not apply to the offense of arson. In cases in which an arson occurs in conjunction with another violent or property crime, both crimes are reported, the arson and the additional crime.\".\n",
    "\n",
    "The goal is to:\n",
    "    1. get a percentage breakdown between violent and non-violent crimes\n",
    "    2. percentage breakdown for violent subcategories\n",
    "    3. analyze dispatch time for above mentioned cases\n",
    "    \n",
    "Columns used:<br>\n",
    "    Class | Four digit code identifying the crime type of the incident<br>\n",
    "    Class Description | Common name description of the incident class type<br> \n",
    "\n",
    "Official reference:\n",
    "https://ucr.fbi.gov/crime-in-the-u.s/2011/crime-in-the-u.s.-2011/violent-crime/violent-crime\n",
    "![alt text](Violent_crime_UCR.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Violent crime class analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pandas to display absolutly all the rows. Usually pandas prints just the beginning and the last part of the df\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.options.display.float_format = '{:20,.2f}'.format\n",
    "\n",
    "# Slice the df on'Class' and 'Class Description'\n",
    "violent_crimes=crimes.loc[:,['Class','Class Description']]\n",
    "\n",
    "# get unique class values & sort ascending\n",
    "print(violent_crimes.sort_values(by='Class').drop_duplicates())\n",
    "\n",
    "# Reset pandas display max rows to default value\n",
    "pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Crimes identified by Class between 111 and 933 could be classified as violent.\n",
    "2. Crimes from 911-933 fall into arson category. Arson as a single crime would not classify as violent but I tend to believe that here there is a combination of crimes which could be classified as violent. \n",
    "3. For the time being I will classify crimes between 111-933 as violent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup violent crime classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes.loc[((crimes['Class']>=111) &(crimes['Class']<=933)),'Violent Crimes']='Violent'\n",
    "crimes.loc[(crimes['Class']>933),'Violent Crimes']='Non-Violent'\n",
    "\n",
    "#print first 5 rows\n",
    "crimes.loc[:,['Class','Class Description','Violent Crimes']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Violent/Non-Violent crime distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting violent/non violent crime counts\n",
    "violent_count=crimes['Violent Crimes'].value_counts()\n",
    "\n",
    "# used the same process as for the time analysis pie chart\n",
    "violent_count=violent_count.to_frame()\n",
    "violent_count.reset_index(inplace=True)\n",
    "violent_count.rename(columns={'index':'labels','Violent Crimes':'values'}, inplace=True)\n",
    "\n",
    "# plot\n",
    "violent_count.iplot(kind='pie', labels='labels',values='values',title='Violent/Non-Violent crime distribution in Montgomery county')\n",
    "violent_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Violent/Non-Violent breakdown:\n",
    "* 42.8% violent crimes; 57.2% non-violent\n",
    "* This is somewhat a little bit of a surprise as violent/non-violent values are quite close. I would have expected  much lower values for non-violent crimes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pie Plotting function:\n",
    "* pie_plot:\n",
    "    * Ref: https://plot.ly/python/pie-charts/\n",
    "    * this type of plot needs a figure and a layout\n",
    "    * both figure and layout are dictionaries\n",
    "    * figure is comprised of:\n",
    "        * domains grid for a particular axis x,y values are between [0,1]\n",
    "        * labels list\n",
    "        * values list\n",
    "    * particular for this code I used zip to iterate over the items mentioned above at the same time & append the items to data\n",
    "    \n",
    "    \n",
    "* make_annotations\n",
    "    * annotations are part of the layout dictionary\n",
    "    * annotations are as well dictionaries\n",
    "    * I decided I didn't want an entire function for the layout but will need one for annotations\n",
    "    * make_annotations requites:\n",
    "        * size - font size fo the text\n",
    "        * text - text associated with the pie chart\n",
    "        * x - positioning of the pie charts on x axis\n",
    "        * y - positioning on y axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pie_plot(domains,labels,values):\n",
    "    data=[]\n",
    "    original = [domains,labels,values]\n",
    "    zipped=tuple([list(tup) for tup in zip(*original)])\n",
    "    for i in range(0,len(zipped)):\n",
    "        data.append({\n",
    "            'labels': zipped[i][1],\n",
    "            'values': zipped[i][2],\n",
    "            'type': 'pie',\n",
    "            'name': 'Starry Night',\n",
    "            'domain': zipped[i][0],\n",
    "            'hoverinfo':'label+percent+name',\n",
    "            'hole': .4,\n",
    "            'pull': .2\n",
    "            })\n",
    "    return data\n",
    "\n",
    "def make_annotations(size,text,x,y):\n",
    "    if len(text)==len(x)==len(y):\n",
    "        data=[]\n",
    "        size=[size]*len(x)\n",
    "        for i,j,a,b in zip(size,text,x,y):\n",
    "            data.append({\n",
    "                \"font\":{\"size\":i},\n",
    "                \"showarrow\": False,\n",
    "                \"text\":j,\n",
    "                \"x\":a,\n",
    "                \"y\":b\n",
    "            })\n",
    "        return data\n",
    "    else:\n",
    "        print('Length mismatch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Violent/Non-Violent crime distribution per PD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice the dataframe based on Police District Number & Violent Crimes\n",
    "vc_pd=crimes.loc[:,['Police District Number','Violent Crimes']]\n",
    "\n",
    "labels=[]\n",
    "values=[]\n",
    "\n",
    "# loopt through each PD\n",
    "for pd in ['1D','2D','3D','4D','5D','6D']:\n",
    "    \n",
    "    # get the violent/non-violent crime counts for each PD\n",
    "    item=vc_pd[vc_pd['Police District Number']==pd]['Violent Crimes'].value_counts()\n",
    "    \n",
    "    # create 2 list from the above step; labels & values\n",
    "    l=item.index.tolist()\n",
    "    v=item.values.tolist()\n",
    "    labels.append(l)\n",
    "    values.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains=[({'x': [0, .20],'y': [0, .49]}),\n",
    "         ({'x': [.35, .55],'y': [0, .49]}),\n",
    "         ({'x': [.70, .90],'y': [0, .49]}),\n",
    "         ({'x': [0, .20],'y': [.50, 3.93]}),\n",
    "         ({'x': [.35, .55],'y': [.50, 3.93]}),\n",
    "         ({'x': [.70, .90],'y': [.50, 3.93]})]\n",
    "          \n",
    "data=[]\n",
    "data=pie_plot(domains,labels,values)\n",
    "\n",
    "size=20\n",
    "text=['PD_1D','PD_2D','PD_3D','PD_4D','PD_5D','PD_6D']\n",
    "x=[0.05,0.45,0.85,0.05,0.45,0.85]\n",
    "y=[1.03,1.03,1.03,0.48,0.48,0.48]\n",
    "anno=make_annotations(size,text,x,y)\n",
    "\n",
    "layout = dict(height = 750,\n",
    "              width = 1000,\n",
    "              autosize = False,\n",
    "              title = 'Violent/Non-Violent crime distribution per Police District',\n",
    "              annotations= anno\n",
    "              )  \n",
    "\n",
    "fig = dict(data=data, layout=layout)\n",
    "iplot(fig, filename='pie-subplots')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PD violent/non-violent breakdown:\n",
    "1D | Rockville:\n",
    "    1. violent 40.8%\n",
    "    2. non-violent 59.2%\n",
    "2D | Bethesda:\n",
    "    1. violent 37.8%\n",
    "    2. non-violent 61.3%\n",
    "3D | Silver Spring:\n",
    "    1. violent 41.6 %\n",
    "    2. non-violent 58.4%\n",
    "4D | Wheaton:\n",
    "    1. violent 39.4%\n",
    "    2. non-violent 60.6%\n",
    "5D | Germantown:\n",
    "    1. violent 50.7%\n",
    "    2. non-violent 49.3 %\n",
    "6D | Gaitersburg:\n",
    "    1. violent 44.5%\n",
    "    2. non-violent 55.5%\n",
    " \n",
    "1. Violent crimes range from 38.7-44.5%\n",
    "2. Non-violent crimes range from 55.4-61.3%\n",
    "3. The only surprise is for PD_5D violent crimes outnumber the non-violent crimes;(violent/non-violent) distribution is 50.7%/49.3%\n",
    "4. highest violent percentage PD_5D 50.7%\n",
    "5. lowest violent percentage PD_2D 38.7%\n",
    "6. highet non-violent percentage PD_2D 61.3%\n",
    "7. lowest non-violent percentage PD_5D 49.3%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Analyze violent/non-violent crime distribution per PD & Place\n",
    "\n",
    "1. Get top 5 places where violent/non-violent crimes are committed\n",
    "2. For each PD create a slice of the dataframe where based on PD number and Crime categorization\n",
    "3. sort values in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a slice of the df based on 'Police District Number','Place','Violent Crimes'\n",
    "crimes_pd_place=crimes.loc[:,['Police District Number','Place','Violent Crimes']]\n",
    "\n",
    "# get violent/non-violent crime counts per PD & Place\n",
    "# 1) Create a new df by grouping the slice based on 'Police District Number','Place'\n",
    "# 2) Chain the df to 'Violent Crimes',get value_counts(),reset the index\n",
    "\n",
    "pd_place_vc=pd.DataFrame({'Counts' : crimes_pd_place.groupby(['Police District Number','Place'])['Violent Crimes'].value_counts()}).reset_index()\n",
    "labels_v=[]\n",
    "values_v=[]\n",
    "\n",
    "labels_nv=[]\n",
    "values_nv=[]\n",
    "\n",
    "# loop throught the PDs\n",
    "for pd in ['1D','2D','3D','4D','5D','6D']:\n",
    "    # get the violent/non-violent crime counts for each PD\n",
    "    item_v=pd_place_vc[(pd_place_vc['Police District Number']==pd)& (pd_place_vc['Violent Crimes']=='Violent')].sort_values(by='Counts',ascending=False).head()\n",
    "    item_nv=pd_place_vc[(pd_place_vc['Police District Number']==pd)& (pd_place_vc['Violent Crimes']=='Non-Violent')].sort_values(by='Counts',ascending=False).head()\n",
    "    \n",
    "    # create labels,values lists for each case i.e violent/non-violent\n",
    "    labels_v.append(item_v['Place'])\n",
    "    values_v.append(item_v['Counts'])\n",
    "    \n",
    "    labels_nv.append(item_nv['Place'])\n",
    "    values_nv.append(item_nv['Counts'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting  most common places where violent crimes are commited /PD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grupped bar subplots plotting functions:\n",
    "1. plot_subplots:\n",
    "\n",
    "    1. Ref:  https://plot.ly/python/subplots/\n",
    "    2. this require a figure & layout\n",
    "    3. the figure is comprised of individual traces and (row; column) specification\n",
    "    4. each trace is made out a x<-->label and y<-->values\n",
    "    5. particular to this function I will use zip to iterate over labels,value,names at the same time & construct the traces\n",
    "    6. make the figure layout\n",
    "    7. append each trace to the figure\n",
    "    \n",
    "    \n",
    "2. make_titles:\n",
    "    1. this will create a list with titles\n",
    "    2. titles are made out a base string + another string typically this would be the PDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_subplots(labels,values,row,col,names,titles):\n",
    "    if len(labels)==len(values)==len(names)==len(titles):\n",
    "        \n",
    "        original = [labels,values,names]\n",
    "        zipped=tuple([list(tup) for tup in zip(*original)])\n",
    "        \n",
    "        trace=[]\n",
    "        for i in range(0,len(labels)):\n",
    "            trace+=['trace'+str(i+1)]\n",
    "            trace[i]=go.Bar(x=zipped[i][0],y=zipped[i][1],name=zipped[i][2])\n",
    "        \n",
    "        fig = tools.make_subplots(\n",
    "            rows=row, cols=col, subplot_titles=(titles)\n",
    "            )\n",
    "        \n",
    "        layout_row=[]\n",
    "        layout_col=[]\n",
    "        \n",
    "        for i in range(1,row+1):\n",
    "            for j in (1,col):\n",
    "                layout_row.append(j)\n",
    "                layout_col.append(i)\n",
    "        \n",
    "        for i,j,k in zip(trace,layout_col,layout_row):\n",
    "            fig.append_trace(i,j,k)\n",
    "        \n",
    "        return fig\n",
    "    else:\n",
    "        return print('Length mismatch')\n",
    "        exit()\n",
    "\n",
    "        \n",
    "def make_titles(names,base_name):\n",
    "    titles=[]\n",
    "    for n in names:\n",
    "        titles+=[str(base_name)+' '+str(n)]\n",
    "    return titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot crime locations distribution/PD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup labels & values\n",
    "labels=labels_v\n",
    "values=values_v\n",
    "\n",
    "# strings used to construct titles\n",
    "names=['PD_1D','PD_2D','PD_3D','PD_4D','PD_5D','PD_6D']\n",
    "base_name='Top 5 violent crime locations'\n",
    "\n",
    "#construct titles list\n",
    "titles=make_titles(names,base_name)\n",
    "\n",
    "row=3;\n",
    "col=2\n",
    "\n",
    "#create figure layout\n",
    "fig=plot_subplots(labels,values,row,col,names,titles)\n",
    "\n",
    "#plot\n",
    "fig['layout'].update(height=1200,width=1000, margin=go.Margin(b=155), title='Top 5 Violent crimes locations/PD')\n",
    "iplot(fig, filename='stacked-subplots-shared-xaxes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 5 violent crime locations:\n",
    "1. The actual places where these crimes are committed are more specific i.e Residence Single Family but I will not focus on that as I fell it will not bring an substantial added value to the analysis\n",
    "2. The order of this locations differ from each PD, however I would not focus on that either\n",
    "3. Most common place where crimes are committed are Residence;Street;Parking Lot;Department/Retail store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The same process as above will follow\n",
    "\n",
    "labels=labels_nv\n",
    "values=values_nv\n",
    "\n",
    "base_name='Top 5 non-violent crime locations'\n",
    "titles=make_titles(names,base_name)\n",
    "\n",
    "row=3;\n",
    "col=2\n",
    "\n",
    "fig=plot_subplots(labels,values,row,col,names,titles)\n",
    "\n",
    "fig['layout'].update(height=1200,width=1000, margin=go.Margin(b=130), title='Top 5 Non-Violent crimes locations/PD')\n",
    "iplot(fig, filename='stacked-subplots-shared-xaxes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 5 non-violent crime locations:\n",
    "1. Most common non-violent crime locations are Residence;Street;Parking Lot;Department/Retail Store\n",
    "2. These are the same locations as for violent crimes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analize violent crimes by main categories\n",
    "\n",
    "1. The goal here is to know what is the percentage of a particular violent crime sub category.\n",
    "2. Classification structure:\n",
    "    1. create a function that is applied against the 'Class' columns i.e(Homicide,Rape...etc)\n",
    "    2. depending on the values the function will return the overall category\n",
    "    3. return the results in a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class2nd(x):\n",
    "    if ((x>100) & (x<=199)):\n",
    "        #print(x)\n",
    "        return 'HOMICIDE'\n",
    "    elif ((x>200) & (x<=299)):\n",
    "        return 'RAPE'\n",
    "    elif ((x>300) & (x<=399)):\n",
    "        return 'ROBBERY'\n",
    "    elif ((x>400)& (x<=499)):\n",
    "        return 'AGG ASSAULT'\n",
    "    elif ((x>500)&(x<=599)):\n",
    "        return 'BURGLARY'\n",
    "    elif ((x>600)&(x<=699)):\n",
    "        return 'LARCENY'\n",
    "    elif ((x>700)&(x<=799)):\n",
    "        return 'AUTO THEFT'\n",
    "    elif ((x>800)&(x<=815)):\n",
    "        return 'ASSAULT & BATTERY'\n",
    "    elif ((x>815)&(x<=825)):\n",
    "        return 'ASSAULT'\n",
    "    elif ((x>900)&(x<999)):\n",
    "        return 'ARSON'\n",
    "    \n",
    "crimes['Class Main Cathegory']=crimes['Class'].apply(class2nd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get violent crimes count by main class category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) create a new df end excluce non-violent crimes\n",
    "# 2) group by PD and get the counts for each violent crime main cathegory \n",
    "\n",
    "violent_main=pd.DataFrame({'Counts': crimes[crimes['Class Main Cathegory'].notnull()==True].groupby(['Police District Number'])['Class Main Cathegory'].value_counts()}).reset_index()\n",
    "\n",
    "#print preview\n",
    "violent_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_v=[]\n",
    "values_v=[]\n",
    "\n",
    "# loop through each PD\n",
    "for pd in ['1D','2D','3D','4D','5D','6D']:\n",
    "    # for each PD get the main cathegory & counts\n",
    "    l=violent_main[violent_main['Police District Number']==pd]['Class Main Cathegory']\n",
    "    v=violent_main[violent_main['Police District Number']==pd]['Counts']\n",
    "    \n",
    "    # create labels & values list\n",
    "    labels_v.append(l)\n",
    "    values_v.append(v)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Plot main violent crimes categories distribution on PDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=labels_v\n",
    "values=values_v\n",
    "\n",
    "base_name='Violent crime distribution'\n",
    "titles=make_titles(names,base_name)\n",
    "\n",
    "row=3\n",
    "col=2\n",
    "# plot\n",
    "fig=plot_subplots(labels,values,row,col,names,titles)\n",
    "\n",
    "fig['layout'].update(height=1200,width=1000, title='Violent Crimes/PD by main category')\n",
    "iplot(fig, filename='stacked-subplots-shared-xaxes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main violent crimes categories distribution on PDs - Pie Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains=[({'x':[0, 0.40], 'y':[0.60,1.0]}), # for the cell (1,1)\n",
    "         ({'x':[0.60, 1], 'y':[0.60,1]}),#cell (1,2)\n",
    "         ({'x':[0, 0.40], 'y':[0.30, 0.66]}), #cell (2,1)\n",
    "         ({'x':[0.60, 1], 'y':[0.30, 0.66]}),#cell (2,2)\n",
    "         ({'x':[0, 0.40], 'y':[0,0.30]}),#cell (3,1)\n",
    "         ({'x': [0.60, 1], 'y':[0,0.30]})]#cell (3,1)\n",
    "\n",
    "data=pie_plot(domains,labels,values)\n",
    "\n",
    "size=24\n",
    "x=[0.14,0.86,0.14,0.86,0.14,0.86]\n",
    "y=[0.82,0.82,0.48,0.48,0.13,0.13]\n",
    "\n",
    "anno=make_annotations(size,text,x,y)\n",
    "layout = dict(height = 1200,\n",
    "              width = 1000,\n",
    "              autosize = False,\n",
    "              title = 'Violent Crimes/PD by main category',\n",
    "              annotations=anno\n",
    "             )\n",
    "\n",
    "fig = dict(data=data, layout=layout)\n",
    "iplot(fig, filename='pie-subplots')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Violent crimes sub categories breakdown:\n",
    "For the most part, the order of the violent crimes in terms of percentage from high to low is the following( did not focus on the exact order): \n",
    "    1. Larceny\n",
    "    2. Burglary\n",
    "    3. ASSAULT & BATTERY\n",
    "    4. ASSAULT\n",
    "    5. AUTO THEFT\n",
    "    6. ROBBERY\n",
    "    7. AGGRAVATED ASSAULT\n",
    "    8. RAPE; ARSON; HOMICIDE\n",
    "1. Larceny:\n",
    "    1. ranges from 59,4%-76,4%\n",
    "    2. highest percent PD_2D\n",
    "    3. lowest percent PD_4D\n",
    "2. Burglary:\n",
    "    1. ranges from 11%-16,2%\n",
    "    2. highest percent PD_1D\n",
    "    3. lowest percent PD_2D\n",
    "3. ASSAULT & BATTERY:\n",
    "    1. ranges from 5.2%-11,4%\n",
    "    2. highest percent PD_4D\n",
    "    3. lowest percent PD_2D\n",
    "4. ASSAULT:\n",
    "    1. ranges from 3.22%-7,1%\n",
    "    2. highest percent PD_5D\n",
    "    3. lowest percent PD_2D\n",
    "5. AUTO THEFT:\n",
    "    1. ranges from 1.87%-5,27%\n",
    "    2. highest percent PD_3D\n",
    "    3. lowest percent PD_2D\n",
    "6. ROBBERY:\n",
    "    1. ranges from 1.75%-5,35%\n",
    "    2. highest percent PD_3D\n",
    "    3. lowest percent PD_2D\n",
    "7. AGGRAVATE ASSAULT:\n",
    "    1. ranges from 0.292%-2,12%\n",
    "    2. highest percent PD_3D\n",
    "    3. lowest percent PD_2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time analysis based on violent/non-violent classification\n",
    "\n",
    "Will attempt to see if there is any difference in dispatch time based on violent/non-violent crime classification.<br>\n",
    "Will also break down the dispatch time based on violent crime sub categories.<br>\n",
    "The assumption is that the more violent the crime is the faster is the dispatch time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#create a new df by filtering out the dates before 01.07.2013 & looking at 'Violent Crimes' and 'Date diff' columns\n",
    "new_td=crimes[crimes['Start Date / Time']>=dt.datetime(2013,7,1)].loc[:,['Violent Crimes','Date diff']].copy()\n",
    "\n",
    "# function defining dispatch time classification\n",
    "def time_class(x):\n",
    "    # assesing time intervals\n",
    "    if x < pd.Timedelta('1 minute'):\n",
    "        return 'Less then 1 minute'\n",
    "    elif ((x>=(pd.Timedelta('1 minute'))) & (x < pd.Timedelta('5 minute'))):\n",
    "        return 'Between 1 and 5'\n",
    "    elif ((x>=(pd.Timedelta('5 minute'))) & (x < pd.Timedelta('10 minute'))):\n",
    "        return 'Between 5 and 10'\n",
    "    elif ((x>=(pd.Timedelta('10 minute'))) & (x < pd.Timedelta('30 minute'))):\n",
    "        return 'Between 10 and 30'\n",
    "    elif ((x>=(pd.Timedelta('30 minute'))) & (x < pd.Timedelta('60 minute'))):\n",
    "        return 'Between 30 and 60'\n",
    "    elif x>pd.Timedelta('60 minute'):\n",
    "        return 'Above 60'\n",
    "\n",
    "def time_analysis_v_nv(df):\n",
    "    labels=[]\n",
    "    values=[]\n",
    "    # loop through violent/non-violent classification\n",
    "    for classif in ['Violent','Non-Violent']: \n",
    "        \n",
    "        #create a new df & apply time classification funtcion then get the counts\n",
    "        td=new_td[new_td['Violent Crimes']==classif].copy()\n",
    "        td['Time Deltas']=td['Date diff'].apply(time_class)\n",
    "        td=td['Time Deltas'].value_counts()\n",
    "        \n",
    "        # convert the series to two label & values lists\n",
    "        l=td.index.tolist()\n",
    "        v=td.values.tolist()\n",
    "        \n",
    "        labels.append(l)\n",
    "        values.append(v)\n",
    "    \n",
    "    return (labels,values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting dispatch time interval based on violent/non-violent classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data list for plotting a pie chart\n",
    "domains=[({\"x\": [0, .40]}),({\"x\": [.52, .92]})]\n",
    "[labels,values]=time_analysis_v_nv(df=new_td)\n",
    "data=pie_plot(domains,labels,values)\n",
    "\n",
    "# creating the layout\n",
    "layout = dict(autosize = True,\n",
    "              margin=go.Margin(b=50),\n",
    "              title=\"Dispatch Time based on violent/non-violent crime classification\",\n",
    "              annotations= [{\"font\": {\"size\": 15},\"showarrow\": False,\"text\": \"Violent Crimes Time Deltas\",\"x\": 0.07, \"y\": .97},\n",
    "                            {\"font\": {\"size\": 15},\"showarrow\": False,\"text\": \"Non-Violent Crimes Time Deltas\",\"x\": 0.90, \"y\": .97}]\n",
    "              )\n",
    "# plot\n",
    "figxx = dict(data=data, layout=layout)\n",
    "iplot(figxx, filename='pie-subplots')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Violent/Non-Violent crime dispatch time breakdown:\n",
    "\n",
    "* less then 1 minute ---> 82.7% decrease from non-violent to violent\n",
    "* between 1 minute and 5 minutes --->27.55 % decrease from non-violent to violent\n",
    "* between 5 minutes and 10 minutes --->41.39% increase from non-violent to violent\n",
    "* between 10 minutes and 30 minutes --->97.08% increase from non-violent to violent\n",
    "* between 30 minutes and 60 minutes --->76.78% increase from non-violent to violent\n",
    "* above 60 minutes --->74.27% increase from non-violent to violent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time delta analysis based on violent class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a data frame for violent crimes\n",
    "td_v=crimes[((crimes['Violent Crimes']=='Violent')& (crimes['Start Date / Time']>=dt.datetime(2013,7,1)))].loc[:,[\"Date diff\",'Class Main Cathegory']]\n",
    "\n",
    "data_l=[]\n",
    "data_v=[]\n",
    "item_d=[]\n",
    "\n",
    "#loop through violent crimes categories \n",
    "for crime in td_v['Class Main Cathegory'].unique():\n",
    "    #construct 2 lists for plotting labels & values\n",
    "    data_l+=[str(crime)+'_l']\n",
    "    data_v+=[str(crime)+'_v']\n",
    "    \n",
    "    # for each violent crime type get dispatch time deltas\n",
    "    item=td_v[td_v['Class Main Cathegory']==crime]['Date diff']\n",
    "    item=item.apply(time_class).value_counts()\n",
    "    \n",
    "    # append lables & values to their corresponding lists\n",
    "    data_l[len(data_l)-1]=item.index\n",
    "    data_v[len(data_v)-1]=item.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains=[({'x':[0, 0.40], 'y':[0.85, 1]}),    # for the cell (1,1)\n",
    "         ({'x':[0.60, 1], 'y':[0.85, 1]}),    # cell (1,2)\n",
    "         ({'x':[0, 0.40], 'y':[0.65, 0.80]}), # cell (2,1)\n",
    "         ({'x':[0.60, 1], 'y':[0.65, 0.80]}), # cell (2,2)\n",
    "         ({'x':[0, 0.40], 'y':[0.45, 0.60]}), # cell (3,1)\n",
    "         ({'x':[0.60, 1], 'y':[0.45, 0.60]}), # cell (3,2)\n",
    "         ({'x':[0, 0.40], 'y':[0.25,  0.40]}),# cell (4,1)\n",
    "         ({'x':[0.60 ,1], 'y':[0.25,  0.40]}),# cell (4,1)\n",
    "         ({'x':[0, 0.40], 'y':[0.05,  0.20]}),# cell (5,1)\n",
    "         ({'x':[0.5, 1], 'y':[0.05,  0.20]})] # cell (5,1)\n",
    "\n",
    "labels=data_l\n",
    "values=data_v\n",
    "\n",
    "data=pie_plot(domains,labels,values)\n",
    "text=td_v['Class Main Cathegory'].unique()\n",
    "text[1]='ASSLT & BTR'\n",
    "text[4]='A THEFT'\n",
    "text[5]='AGG ASSLT'\n",
    "size=15\n",
    "x=[0.15,0.875,0.14,0.865,0.15,0.875,0.15,0.84,0.16,0.81]\n",
    "y=[0.93,0.93,0.73,0.73,0.52,0.52,0.32,0.32,0.12,0.12]\n",
    "\n",
    "anno=make_annotations(size,text,x,y)\n",
    "layout = dict(height = 1600,\n",
    "              width = 1000,\n",
    "              autosize = True,\n",
    "              margin=go.Margin(b=20),\n",
    "              title=\"Dispatch Time based on violent crime classification\",\n",
    "              annotations=anno\n",
    "             )\n",
    "figxx = dict(data=data, layout=layout)\n",
    "iplot(figxx, filename='pie-subplots')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Violent crimes dispatch time breakdown:\n",
    "1. LARCENY:\n",
    "    1. biggest dispatch time category: above 60 minutes 63%\n",
    "    2. smallest dispatch time category: between 1 and 5 minutes 0.762%\n",
    "    \n",
    "    \n",
    "2. ASSAULT & BATTERY:\n",
    "    1. biggest dispatch time category: less then 1 minute 61%\n",
    "    2. smallest dispatch time category: between 1 and 5 minutes 3.09%\n",
    "\n",
    "\n",
    "3. BURGLARY:\n",
    "    1. biggest dispatch time category: above 60 minutes 65.6%\n",
    "    2. smallest dispatch time category: between 30 and 60 minutes 1.1%\n",
    "\n",
    "\n",
    "4. ROBBERY:\n",
    "    1. biggest dispatch time category: less then 1 minute 43.8%\n",
    "    2. smallest dispatch time category: between 30 and 60 minutes 4.61%\n",
    "\n",
    "\n",
    "5. AUTO THEFT:\n",
    "    1. biggest dispatch time category: above 60 minutes 71%\n",
    "    2. smallest dispatch time category: between 5 and 10 minutes 0.763%\n",
    "\n",
    "\n",
    "6. AGGRAVATE ASSAULT:\n",
    "    1. biggest dispatch time category: less then 1 minute 64.1%\n",
    "    2. smallest dispatch time category: between 1 and 5 minutes 3.59%\n",
    "\n",
    "\n",
    "7. ASSAULT:\n",
    "    1. biggest dispatch time category: less then 1 minute 62.3%\n",
    "    2. smallest dispatch time category: between 1 and 5 minutes 2.16%\n",
    "\n",
    "\n",
    "8. RAPE:\n",
    "    1. biggest dispatch time category: above 60 minutes 65.7%\n",
    "    2. smallest dispatch time category: between 30 and 60 minutes 2.86%\n",
    "\n",
    "\n",
    "9. ARSON:\n",
    "    1. biggest dispatch time category: less then 1 minutes 67.9%\n",
    "    2. smallest dispatch time category: between 30 and 60 minutes 3.57%\n",
    "\n",
    "\n",
    "10. HOMICIDE:\n",
    "    1. biggest dispatch time category: less then 1 minutes 75.%\n",
    "    2. smallest dispatch time category: between 1 and 5 minutes 25%\n",
    "\n",
    "\n",
    "11. DISPAtCH TIME DELTAS CATEGORY BREAKDOWN:\n",
    "    1. less than 1 minutes:\n",
    "        1. highest % HOMICIDE 75%\n",
    "        2. lowest  % AUTO THEFT 22.1%   \n",
    "    2. between 1 minute and 5 minutes:\n",
    "        1. highest % HOMICIDE 25%\n",
    "        2. lowest  % LARCENY 0.762%\n",
    "    3. between 5 minutes and 10 minutes:\n",
    "        1. highest % ROBBERY 11.8%\n",
    "        2. lowest  % AUTO THEFT 0.763\n",
    "    4. between 10 minutes and 30 minutes:\n",
    "        1. highest % ROBBERY 17%\n",
    "        2. lowest  % AUTO THEFT 2.54%\n",
    "    5. between 30 minutes and 60 minutes:\n",
    "        1. highest % ROBBERY 4.61%\n",
    "        2. lowest  % BURGLARY 1.1%\n",
    "    6. above 60 minutes:\n",
    "        1. highest % AUTO THEFT 71%\n",
    "        2. lowest  % ASSAULT & BATTERY 13.3%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map visualization\n",
    "Having a map representation of the crime distribution can make the data more compelling to a casual reader.<br>\n",
    "With this in mind i used folium to plot crime on Montgomery district.<br>\n",
    "Columns used:\n",
    "    1. Latitude\n",
    "    2. Longitude "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### folium plotting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def folium_plot(data_out,data):\n",
    "    \n",
    "    # setup county boundries\n",
    "    map = folium.Map(location=[39.154743, -77.240515], zoom_start=10.5)\n",
    "    \n",
    "    map.choropleth(geo_path = district_geo,  \n",
    "                  data_out = data_out,        # json file with PD boundries\n",
    "                  data=data,                  # df used for plotting\n",
    "                  columns = ['DIST', 'Number'],\n",
    "                  key_on = 'feature.properties.DIST',  # bind the json and data file on district\n",
    "                  fill_color='YlGn', fill_opacity=0.8, line_opacity=0.2,\n",
    "                  legend_name='Crime Vizualization')\n",
    "    return map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "# Assigning the geojson file to a variable\n",
    "district_geo = r'montgomery_county_pd.geojson'\n",
    "\n",
    "# Preparing the data for plotting; this also means reordering the index to match the geojson file\n",
    "crimedata2 = pd.DataFrame(crimes['Police District Number'].value_counts().astype(float))[:6]\n",
    "crimedata2.index = [3,4,6,1,2,5]\n",
    "\n",
    "# Cleaned data to json\n",
    "crimedata2.to_json('crimeagg_new.json')\n",
    "\n",
    "# Reset index; rename columns\n",
    "crimedata2 = crimedata2.reset_index()\n",
    "\n",
    "crimedata2.columns = ['DIST', 'Number']\n",
    "# Initiate folium map and then plot it\n",
    "\n",
    "map=folium_plot(data_out='crimeagg_new.json',data=crimedata2)\n",
    "map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. This map representation is a little bit misleading in the sense that it does not take into account the population percentage.\n",
    "2. With this in mind let's do the same thing but try to get the percentage by/100K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "district_geo = r'montgomery_county_pd.geojson'\n",
    "\n",
    "crimedata=crimes['Police District Number'].value_counts()\n",
    "crimedata=crimedata.iloc[:6]\n",
    "\n",
    "crimedata=crimedata.to_frame().reset_index()\n",
    "crimedata.columns=['Police District Number','Number']\n",
    "crimedata.index=[3,4,6,1,2,5]\n",
    "\n",
    "#get the crime percentage per 100K\n",
    "census.index=[1,2,3,4,5,6]\n",
    "crimedata['Number']=crimedata['Number']*100/census['Population'].astype(float)\n",
    "\n",
    "json_conv=pd.DataFrame({'Police District Number':crimedata['Number']},index=[3,4,6,1,2])\n",
    "json_conv.to_json('crimeaggcc.json')\n",
    "\n",
    "\n",
    "crimedata=crimedata.loc[:,['Police District Number','Number']]\n",
    "crimedata.columns=['DIST','Number']\n",
    "crimedata=crimedata.reset_index(drop=True)\n",
    "crimedata['DIST']=crimedata['DIST'].str.strip('D').astype(float)\n",
    "\n",
    "map=folium_plot(data_out='crimeaggcc.json',data=crimedata)\n",
    "map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Conclusions\n",
    "\n",
    "Analysis conducted on this dataset has 3 directions: time analysis, location analysis and crime classification analysis\n",
    "###### Time analysis:\n",
    "1. Week:\n",
    "    2. most of the crimes are committed Tuesday and least crimes are Sunday\n",
    "\t3. there is a general decreasing trends towards the end of the weekly<br><br>\n",
    "2. Hour:\n",
    "    1. most of the crimes are committed between 7am -11pm and least crimes between 5am-7am<br><br>\n",
    "3. Month:\n",
    "    1. highest crimes rates are in October\n",
    "    2. lowest crimes rate in July\n",
    "\t3. amendment dataset is not complete as it contains timep stamps from July onward<br><br>\n",
    "4. Dispatch Time conclusion:\n",
    "\t1. highest dispatch time interval is less then 1 min 48.4% followed by a dispatch time above 60 min 41%\n",
    "\t2. smallest dispatch category is between 1 min and  5 min   1.72%<br><br>\n",
    "5. Dispatch time deltas based on violent crime subcategories:\n",
    "    1. less than 1 minutes:\n",
    "        * highest % HOMICIDE 75%\n",
    "        * lowest % AUTO THEFT 22.1%<br><br>\n",
    "    2. between 1 minute and 5 minutes:\n",
    "        * highest % HOMICIDE 25%\n",
    "        * lowest % LARCENY 0.762%<br><br>\n",
    "    3. between 5 minutes and 10 minutes:\n",
    "        * highest % ROBBERY 11.8%\n",
    "        * lowest % AUTO THEFT 0.763<br><br>\n",
    "    4. between 10 minutes and 30 minutes:\n",
    "        * highest % ROBBERY 17%\n",
    "        * lowest % AUTO THEFT 2.54%<br><br>\n",
    "    5. between 30 minutes and 60 minutes:\n",
    "        * highest % ROBBERY 4.61%\n",
    "        * lowest % BURGLARY 1.1%<br><br>\n",
    "    6. above 60 minutes:\n",
    "        * highest % AUTO THEFT 71%\n",
    "        * lowest % ASSAULT & BATTERY 13.3%\n",
    "        \n",
    "###### Location analysis:\n",
    "Locations can vary from a size stand point i.e Police District; City; Location(Street;Residence)<br>\n",
    "1.Police District:<br>\n",
    "    1. 3D | Silver Spring 24.8%\n",
    "    2. 6D | Gaithersburg 17.6%\n",
    "    3. 1D | Rockville 16%\n",
    "    4. 4D | Wheaton 14.5%\n",
    "    5. 5D | Germantown 14.3%\n",
    "    6. 2D | Bethesda 12.7%\n",
    "     \n",
    "2. Locations:\n",
    "\t1. most common:\n",
    "        * Residence; Street ;Parking Lot\n",
    "    2. least common:\n",
    "\t\t* Jail; Retail; Liquor Store; Lake; Pawn Shop; Residence(Mobile Home); Nursery; Parking Lot(Park & Ride)\n",
    "\n",
    "##### Crime classification analysis:\n",
    "1. Montgomery county:\n",
    "    1. overall violent and non-violent crimes have close values (42.8% violent crimes; 57.2% non-violent) which comes as a surprise\n",
    "\t2. violent crimes range from 38.7-44.5%\n",
    "    3. non-violent crimes range from 55.4-61.3%\n",
    "         \n",
    "         \n",
    "2. Police Districts crime percentage:\t\n",
    "    1. highest violent percentage 5D - Germantown 50.7%\n",
    "\t2. lowest violent percentage 2D - Bethesda 38.7%\n",
    "\t3. highest non-violent percentage 2D - Bethesda 61.3%\n",
    "\t4. lowest non-violent percentage 5D - Germantown 49.3%\n",
    "\n",
    "\n",
    "3. Violent Crimes subcategory analysis:\n",
    "    1. Larceny ranges from 59,4%-76,4%\n",
    "    2. Burglary ranges from 11%-16,2%\n",
    "\t3. ASSAULT & BATTERY ranges from 5.2%-11,4%\n",
    "\t4. ASSAULT ranges from 3.22%-7,1%\n",
    "    5. AUTO THEFT ranges from 1.87%-5,27%\n",
    "\t6. ROBBERY ranges from 1.75%-5,35%\n",
    "\t7. AGGRAVATE ASSAULT ranges from 0.292%-2,12%\n",
    "\n",
    "\n",
    "4. Violent crime classification dispatch time breakdown:     \n",
    "    1. less then 1 minute ---> 82.7% decrease from non-violent to violent\n",
    "\t2. between 1 minute and 5 minutes --->27.55 % decrease from non-violent to violent\n",
    "\t3. between 5 minutes and 10 minutes --->41.39% increase from non-violent to violent\n",
    "    4. between 10 minutes and 30 minutes --->97.08% increase from non-violent to violent\n",
    "    5. between 30 minutes and 60 minutes --->76.78% increase from non-violent to violent\n",
    "\t6. above 60 minutes --->74.27% increase from non-violent to violent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
